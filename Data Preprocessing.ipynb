{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Fake News Detection  \n",
    "\n",
    "This notebook is dedicated to the preprocessing of datasets for the **Fake News Detection Project**. It includes tasks like:  \n",
    "- Cleaning raw datasets.  \n",
    "- Aligning column structures.  \n",
    "- Handling missing or irrelevant data.  \n",
    "- Standardizing data formats for analysis.  \n",
    "- Preparing the datasets for machine learning and statistical evaluation.  \n",
    "\n",
    "The preprocessing ensures consistency across datasets, paving the way for effective modeling and analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "BuzzFeed Fake News: (91, 12)\n",
      "BuzzFeed Real News: (91, 12)\n",
      "LIAR Test Set: (1267, 14)\n",
      "LIAR Train Set: (10240, 14)\n",
      "LIAR Validation Set: (1284, 14)\n",
      "PolitiFact Factcheck: (21152, 8)\n",
      "Fake News Corpus: (249, 16)\n"
     ]
    }
   ],
   "source": [
    "# Libraries and Dataset Loading\n",
    "# Step 1: Importing Necessary Libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Loading the Datasets\n",
    "# BuzzFeed FakeNewsNet Dataset\n",
    "buzzfeed_fake_df = pd.read_csv('Datasets/BuzzFeed_fake_news_content.csv')\n",
    "buzzfeed_real_df = pd.read_csv('Datasets/BuzzFeed_real_news_content.csv')\n",
    "\n",
    "# LIAR Datasets\n",
    "test_df = pd.read_csv('Datasets/test.csv')\n",
    "train_df = pd.read_csv('Datasets/train.csv')\n",
    "valid_df = pd.read_csv('Datasets/valid.csv')\n",
    "\n",
    "# PolitiFact Factcheck Dataset\n",
    "politifact_factcheck_df = pd.read_csv('Datasets/politifact_factcheck.csv')\n",
    "\n",
    "# Fake News Corpus Dataset\n",
    "fnc_df = pd.read_csv('Datasets/Fake News Corpus.csv')\n",
    "\n",
    "# Step 3: Display Dataset Shapes\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"BuzzFeed Fake News: {buzzfeed_fake_df.shape}\")\n",
    "print(f\"BuzzFeed Real News: {buzzfeed_real_df.shape}\")\n",
    "print(f\"LIAR Test Set: {test_df.shape}\")\n",
    "print(f\"LIAR Train Set: {train_df.shape}\")\n",
    "print(f\"LIAR Validation Set: {valid_df.shape}\")\n",
    "print(f\"PolitiFact Factcheck: {politifact_factcheck_df.shape}\")\n",
    "print(f\"Fake News Corpus: {fnc_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buzzfeed Dataset Processed\n",
      "     id                                              title  \\\n",
      "0   BF1  Proof The Mainstream Media Is Manipulating The...   \n",
      "1  BF10  Charity: Clinton Foundation Distributed “Water...   \n",
      "2  BF11  A Hillary Clinton Administration May be Entire...   \n",
      "3  BF12  Trump’s Latest Campaign Promise May Be His Mos...   \n",
      "4  BF13                    Website is Down For Maintenance   \n",
      "\n",
      "                                                text  \\\n",
      "0  I woke up this morning to find a variation of ...   \n",
      "1  Former President Bill Clinton and his Clinton ...   \n",
      "2  After collapsing just before trying to step in...   \n",
      "3  Donald Trump is, well, deplorable. He’s sugges...   \n",
      "4                    Website is Down For Maintenance   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://www.addictinginfo.org/2016/09/19/proof-...   \n",
      "1  http://eaglerising.com/36899/charity-clinton-f...   \n",
      "2  http://eaglerising.com/36880/a-hillary-clinton...   \n",
      "3  http://www.addictinginfo.org/2016/09/19/trumps...   \n",
      "4  http://www.proudcons.com/clinton-foundation-ca...   \n",
      "\n",
      "                       speaker  label  \n",
      "0              Wendy Gittleson  False  \n",
      "1               View All Posts  False  \n",
      "2  View All Posts,Tony Elliott  False  \n",
      "3                  John Prager  False  \n",
      "4                      Unknown  False  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean and process the BuzzFeed datasets\n",
    "\n",
    "# Cleaning function for BuzzFeed datasets\n",
    "def clean_buzzfeed_dataset(df, label, prefix):\n",
    "    # Keep only relevant columns\n",
    "    keep_columns = ['id', 'title', 'text', 'url', 'authors']\n",
    "    df = df[keep_columns].copy()\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'authors': 'speaker'}, inplace=True)\n",
    "    \n",
    "    # Add label column\n",
    "    df['label'] = label\n",
    "    \n",
    "    # Modify 'id' column\n",
    "    df['id'] = df['id'].str.extract(r'(\\d+)').astype(str)  # Extract numerical part\n",
    "    df['id'] = prefix + df['id']  # Add prefix (BF or BR)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 1: Clean both datasets\n",
    "buzzfeed_fake_cleaned = clean_buzzfeed_dataset(buzzfeed_fake_df, 'False', 'BF')\n",
    "buzzfeed_real_cleaned = clean_buzzfeed_dataset(buzzfeed_real_df, 'True', 'BR')\n",
    "\n",
    "# Step 2: Combine the cleaned datasets\n",
    "buzzfeed_combined = pd.concat([buzzfeed_fake_cleaned, buzzfeed_real_cleaned], ignore_index=True)\n",
    "\n",
    "# Step 3: Replace missing values explicitly with \"Unknown\"\n",
    "buzzfeed_combined = buzzfeed_combined.fillna(value=\"Unknown\")\n",
    "\n",
    "# Step 4: Save the combined dataset\n",
    "buzzfeed_combined.to_csv('Buzzfeed.csv', index=False)\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"Buzzfeed Dataset Processed\")\n",
    "print(buzzfeed_combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Label Counts in LIAR Dataset:\n",
      "label\n",
      "partially-true    7184\n",
      "False             2507\n",
      "True              2053\n",
      "exaggerated       1047\n",
      "Name: count, dtype: int64\n",
      "LIAR Dataset Processed and Saved\n",
      "           id           label  \\\n",
      "0  LIAR-11972            True   \n",
      "1  LIAR-11685           False   \n",
      "2  LIAR-11096           False   \n",
      "3   LIAR-5209  partially-true   \n",
      "4   LIAR-9524     exaggerated   \n",
      "\n",
      "                                               title  \\\n",
      "0  Building a wall on the U.S.-Mexico border will...   \n",
      "1  Wisconsin is on pace to double the number of l...   \n",
      "2  Says John McCain has done nothing to help the ...   \n",
      "3  Suzanne Bonamici supports a plan that will cut...   \n",
      "4  When asked by a reporter whether hes at the ce...   \n",
      "\n",
      "                            Speaker  \n",
      "0                        rick-perry  \n",
      "1                 katrina-shankland  \n",
      "2                      donald-trump  \n",
      "3                     rob-cornilles  \n",
      "4  state-democratic-party-wisconsin  \n"
     ]
    }
   ],
   "source": [
    "# Define the function to clean and process the LIAR dataset\n",
    "\n",
    "def clean_liar_dataset(df):\n",
    "    # Keep only relevant columns\n",
    "    keep_columns = ['ID', 'Label', 'Statement', 'Speaker']\n",
    "    df = df[keep_columns].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'ID': 'id', 'Label': 'label', 'Statement': 'title'}, inplace=True)\n",
    "\n",
    "    # Prefix \"LIAR-\" to all values in the id column\n",
    "    df['id'] = df['id'].apply(lambda x: f\"LIAR-{x}\")\n",
    "\n",
    "    # Replace missing values with 'unknown'\n",
    "    df.fillna('unknown', inplace=True)\n",
    "\n",
    "    # Apply smart mapping for labels\n",
    "    label_mapping = {\n",
    "        'true': 'True',\n",
    "        'TRUE': 'True',\n",
    "        'false': 'False',\n",
    "        'FALSE': 'False',\n",
    "        'mostly-false': 'False',\n",
    "        'half-true': 'partially-true',\n",
    "        'mostly-true': 'partially-true',\n",
    "        'barely-true': 'partially-true',\n",
    "        'pants-fire': 'exaggerated',\n",
    "        'unknown': 'unknown'\n",
    "    }\n",
    "\n",
    "    # Normalize the labels and map them using the label_mapping\n",
    "    df['label'] = df['label'].str.lower().map(label_mapping).fillna('unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 1: Clean and process all three datasets (train, test, valid)\n",
    "test_cleaned = clean_liar_dataset(test_df)\n",
    "train_cleaned = clean_liar_dataset(train_df)\n",
    "valid_cleaned = clean_liar_dataset(valid_df)\n",
    "\n",
    "# Step 2: Combine all cleaned datasets into one\n",
    "liar_combined = pd.concat([test_cleaned, train_cleaned, valid_cleaned], ignore_index=True)\n",
    "\n",
    "# Step 3: Display the updated label counts for verification\n",
    "print(\"Updated Label Counts in LIAR Dataset:\")\n",
    "print(liar_combined['label'].value_counts())\n",
    "\n",
    "# Step 4: Save the combined dataset to CSV\n",
    "liar_combined.to_csv('LIAR.csv', index=False)\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"LIAR Dataset Processed and Saved\")\n",
    "print(liar_combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politifact Factcheck Dataset Processed\n",
      "    id           label       speaker  \\\n",
      "0  PF1            True  Barack Obama   \n",
      "1  PF2           False    Matt Gaetz   \n",
      "2  PF3  partially-true  Kelly Ayotte   \n",
      "3  PF4           False      Bloggers   \n",
      "4  PF5  partially-true  Bobby Jindal   \n",
      "\n",
      "                                               title       date  \\\n",
      "0  John McCain opposed bankruptcy protections for...  6/16/2008   \n",
      "1  \"Bennie Thompson actively cheer-led riots in t...  6/13/2022   \n",
      "2  Says Maggie Hassan was \"out of state on 30 day...  5/27/2016   \n",
      "3  \"BUSTED: CDC Inflated COVID Numbers, Accused o...   2/5/2021   \n",
      "4  \"I'm the only (Republican) candidate that has ...  8/30/2015   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.politifact.com/factchecks/2008/jun...  \n",
      "1  https://www.politifact.com/factchecks/2022/jun...  \n",
      "2  https://www.politifact.com/factchecks/2016/may...  \n",
      "3  https://www.politifact.com/factchecks/2021/feb...  \n",
      "4  https://www.politifact.com/factchecks/2015/aug...  \n"
     ]
    }
   ],
   "source": [
    "# Define the function to clean and process the Politifact Factcheck dataset\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "politifact_factcheck_df = pd.read_csv('Datasets/politifact_factcheck.csv')\n",
    "\n",
    "# Step 2: Select only the required columns\n",
    "keep_columns = ['verdict', 'statement_originator', 'statement', 'factcheck_date', 'factcheck_analysis_link']\n",
    "politifact_cleaned = politifact_factcheck_df[keep_columns].copy()\n",
    "\n",
    "# Step 3: Rename the columns\n",
    "politifact_cleaned.rename(columns={\n",
    "    'verdict': 'label',\n",
    "    'statement_originator': 'speaker',\n",
    "    'statement': 'title',\n",
    "    'factcheck_date': 'date',\n",
    "    'factcheck_analysis_link': 'url'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 4: Add a new 'id' column with sequential values prefixed with 'PF'\n",
    "politifact_cleaned.insert(0, 'id', ['PF' + str(i) for i in range(1, len(politifact_cleaned) + 1)])\n",
    "\n",
    "# Step 5: Replace missing values with 'unknown'\n",
    "politifact_cleaned = politifact_cleaned.fillna('unknown')\n",
    "\n",
    "# Step 6: Apply smart mapping for the 'label' column\n",
    "label_mapping = {\n",
    "    'true': 'True',\n",
    "       # 'True': 'TRUE',\n",
    "        'TRUE': 'True',\n",
    "        'false': 'False',\n",
    "       # 'False': 'FALSE',\n",
    "        'FALSE': 'False',\n",
    "        'mostly-false': 'False',\n",
    "        'half-true': 'partially-true',\n",
    "        'mostly-true': 'partially-true',\n",
    "        'barely-true': 'partially-true',\n",
    "        'pants-fire': 'exaggerated',\n",
    "        'unknown': 'unknown'\n",
    "}\n",
    "politifact_cleaned['label'] = politifact_cleaned['label'].str.lower().map(label_mapping).fillna('unknown')\n",
    "\n",
    "# Step 7: Save the cleaned dataset\n",
    "politifact_cleaned.to_csv('Politifact.csv', index=False)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"Politifact Factcheck Dataset Processed\")\n",
    "print(politifact_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Corpus Dataset Processed\n",
      "       id                                              title  \\\n",
      "0  FNC141  Church Congregation Brings Gift to Waitresses ...   \n",
      "1  FNC256  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
      "2  FNC700  Never Hike Alone - A Friday the 13th Fan Film ...   \n",
      "3  FNC768  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...   \n",
      "4  FNC791  Trump’s Genius Poll Is Complete & The Results ...   \n",
      "\n",
      "                                                text        label  \\\n",
      "0  Sometimes the power of Christmas will make you...        False   \n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...        False   \n",
      "2  Never Hike Alone: A Friday the 13th Fan Film U...        False   \n",
      "3  When a rare shark was caught, scientists were ...        False   \n",
      "4  Donald Trump has the unnerving ability to abil...  exaggerated   \n",
      "\n",
      "           speaker                                                url  \n",
      "0      Ruth Harris  http://awm.com/church-congregation-brings-gift...  \n",
      "1     Zurich Times  http://beforeitsnews.com/awakening-start-here/...  \n",
      "2          unknown  http://www.cnnnext.com/video/18526/never-hike-...  \n",
      "3  Alexander Smith  http://awm.com/elusive-alien-of-the-sea-caught...  \n",
      "4  Gloria Christie  http://bipartisanreport.com/2018/01/21/trumps-...  \n"
     ]
    }
   ],
   "source": [
    "# Define the function to clean and process the Fake News Corpus dataset\n",
    "\n",
    "# Step 1: Select only the required columns\n",
    "keep_columns = ['id', 'type', 'url', 'content', 'title', 'authors']\n",
    "fnc_cleaned = fnc_df[keep_columns].copy()\n",
    "\n",
    "# Step 2: Rename the columns\n",
    "fnc_cleaned.rename(columns={\n",
    "    'type': 'label',\n",
    "    'content': 'text',\n",
    "    'authors': 'speaker'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 3: Modify 'id' column by adding a prefix 'FNC' to the existing id values\n",
    "fnc_cleaned['id'] = 'FNC' + fnc_cleaned['id'].astype(str)\n",
    "\n",
    "# Step 4: Reorder the columns as per the new structure\n",
    "fnc_cleaned = fnc_cleaned[['id', 'title', 'text', 'label', 'speaker', 'url']]\n",
    "\n",
    "# Step 5: Replace missing values with 'unknown'\n",
    "fnc_cleaned = fnc_cleaned.fillna('unknown')\n",
    "\n",
    "# Step 6: Apply the label mapping\n",
    "label_mapping = {\n",
    "    'true': 'True',\n",
    "        'TRUE': 'True',\n",
    "        'false': 'False',\n",
    "        'FALSE': 'False',\n",
    "        'mostly-false': 'False',\n",
    "        'half-true': 'partially-true',\n",
    "        'mostly-true': 'partially-true',\n",
    "        'barely-true': 'partially-true',\n",
    "        'pants-fire': 'exaggerated',\n",
    "        'unknown': 'unknown'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "fnc_cleaned['label'] = fnc_cleaned['label'].map(label_mapping).fillna('unknown')\n",
    "\n",
    "# Step 7: Save the cleaned dataset\n",
    "fnc_cleaned.to_csv('FNC.csv', index=False)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"Fake News Corpus Dataset Processed\")\n",
    "print(fnc_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Dataset Created\n",
      "     id                                              title  \\\n",
      "0   BF1  Proof The Mainstream Media Is Manipulating The...   \n",
      "1  BF10  Charity: Clinton Foundation Distributed “Water...   \n",
      "2  BF11  A Hillary Clinton Administration May be Entire...   \n",
      "3  BF12  Trump’s Latest Campaign Promise May Be His Mos...   \n",
      "4  BF13                    Website is Down For Maintenance   \n",
      "\n",
      "                                                text  label  \\\n",
      "0  I woke up this morning to find a variation of ...  False   \n",
      "1  Former President Bill Clinton and his Clinton ...  False   \n",
      "2  After collapsing just before trying to step in...  False   \n",
      "3  Donald Trump is, well, deplorable. He’s sugges...  False   \n",
      "4                    Website is Down For Maintenance  False   \n",
      "\n",
      "                       speaker     date  \\\n",
      "0              Wendy Gittleson  unknown   \n",
      "1               View All Posts  unknown   \n",
      "2  View All Posts,Tony Elliott  unknown   \n",
      "3                  John Prager  unknown   \n",
      "4                      Unknown  unknown   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.addictinginfo.org/2016/09/19/proof-...  \n",
      "1  http://eaglerising.com/36899/charity-clinton-f...  \n",
      "2  http://eaglerising.com/36880/a-hillary-clinton...  \n",
      "3  http://www.addictinginfo.org/2016/09/19/trumps...  \n",
      "4  http://www.proudcons.com/clinton-foundation-ca...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load All Preprocessed Fake News Datasets\n",
    "buzzfeed_df = pd.read_csv('Buzzfeed.csv')\n",
    "liar_df = pd.read_csv('LIAR.csv')\n",
    "politifact_df = pd.read_csv('Politifact.csv')\n",
    "fnc_df = pd.read_csv('FNC.csv')  \n",
    "\n",
    "# Step 2: Standardize Column Structure Across All Datasets\n",
    "final_columns = ['id', 'title', 'text', 'label', 'speaker', 'date', 'url']\n",
    "\n",
    "# Utility Function: Ensure All Datasets Contain Required Columns\n",
    "def ensure_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 'unknown'  \n",
    "    return df[columns]\n",
    "\n",
    "# Apply Standardization to Each Dataset\n",
    "buzzfeed_df = ensure_columns(buzzfeed_df, final_columns)\n",
    "liar_df = ensure_columns(liar_df, final_columns)\n",
    "politifact_df = ensure_columns(politifact_df, final_columns)\n",
    "fnc_df = ensure_columns(fnc_df, final_columns)  \n",
    "\n",
    "# Step 3: Merge All Standardized Datasets into a Unified Dataset\n",
    "fake_news_dataset = pd.concat([buzzfeed_df, liar_df, politifact_df, fnc_df], ignore_index=True)\n",
    "\n",
    "# Step 5: Save the Final Unified and Labeled Dataset to CSV\n",
    "fake_news_dataset.to_csv('Fake_News_Dataset.csv', index=False)\n",
    "\n",
    "# Step 6: Output a Preview of the Final Unified Dataset\n",
    "print(\"Fake News Dataset Created\")\n",
    "print(fake_news_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (34374, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", fake_news_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
